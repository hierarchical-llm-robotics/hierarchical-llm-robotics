<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>hierarchical-llm-robotics</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EDF010G6PN"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EDF010G6PN');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
  <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css"/>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/zcswdt">
            DRaMa
          </a>
        </div>
      </div>
    </div>
  </div>
</nav> -->

</body>
</html>



<style>
  .author-block {
    display: flex;
    align-items: center; /* 让内容垂直居中 */
  }

  .author-block sup {
    font-size: 0.8em; /* 缩小 * 号 */
    margin-left: 2px; /* 添加一点左侧间距 */
    vertical-align: top; /* 让 * 号靠顶部对齐 */
  }
</style>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="container has-text-centered">
        <h1 class="title is-1 publication-title">
  Hierarchical LLM-Guided Manipulation with <br> Multimodal Learning and Action-Mask Policies
</h1>
        <div class="is-size-5 publication-authors" style="display: flex; justify-content: center; gap: 30px;">

      

        </div>
      </div>
    </div>
  </div>
</section>






<div class="column has-text-centered">
  <div class="publication-links">

    
    <span class="link-block">
      <a href=""
         class="external-link button is-normal is-rounded is-dark" target="_blank">
        <span class="icon">
            <i class="fas fa-file-pdf"></i>
        </span>
        <span>Paper (coming Soon)</span>
      </a>
    </span>


    <!-- ML Code Link -->
    <span class="link-block">
      <a href="https://anonymous.4open.science/r/sonicaloha_ml-2832"
         class="external-link button is-normal is-rounded is-dark">
        <span class="icon">
          <i class="fab fa-github"></i>
        </span>
        <span>Code</span>
      </a>
    </span>
  </div>
</div>


      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
 Hierarchical policies that integrate high-level planning with low-level control have shown strong performance in robotic manipulation, but remain limited. 
    We present a hierarchical framework that combines a two-stage task planner with a multimodal low-level action-mask policy. 
    At the high level, a Vision-Language Model (VLM) first perceives object and scene information from observations, and a Large Language Model (LLM) then reasons over this information together with a task library and human instructions to generate a textual task plan. 
    This two-stage design mitigates modality bias.
    At the low level, we employ an asymmetric encoder, using SigLIP2 with Weight-Decomposed Low-Rank Adaptation (DoRA) for text and ResNets for multi-view vision. 
    We introduce a shared Temperature-Scaled Spatial Attention module to enhance multi-view features and a Bidirectional Cross-Attention module to fuse language-vision features for an Action Chunking Transformer (ACT) policy. 
    For multi-task switching, we propose a novel explicit action-mask policy that jointly predicts actions and their validity masks. 
    By explicitly supervising termination, the policy learns not only fine-grained control but also when to stop, enabling real-time sub-task completion detection and robust switching across long-horizon tasks without additional inference overhead. 
    Experiments on weighing and multi-object manipulation scenarios demonstrate planning accuracy, execution success, and efficiency, with ablations confirming the contribution of each component. 
    Finally, deployment on a different robotic platform in a new scenario validates generalization.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<hr/>


<div style="margin-bottom: 40px;"></div>

<section class="hero teaser">
  <div class="hero-body" style="background: linear-gradient(135deg, #56CCF2, #2F80ED); padding: 20px 0;">
    <div class="container is-max-desktop" style="text-align: center;">
      <h2 class="title is-3" style="color: #FFFFFF; margin: 0;">
        Human-Language-Guided Multimodal Autonomous Bimanual Multi-Task Robotic Manipulation
      </h2>
      <p style="color: #E0E0E0; font-size: 1em; margin-top: 6px;">
        Demos include weighing, multi-object, and drawer manipulation scenarios
      </p>
      <p style="color: #E0E0E0; font-size: 1em; margin-top: 6px;">
        Covering human instructions of varying difficulty levels
      </p>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div style="height: 20px;"></div> <!-- 间隔 -->

      <div style="display: flex; flex-wrap: wrap; justify-content: center;">
        <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; margin-bottom: 20px;">
          <video autoplay controls muted loop playsinline 
                 style="width: 100%; max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
            <source src="./static/videos/LLMsAloha_1-compress.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>
  </div>
</section>




<section class="hero teaser">
  <div class="hero-body" style="background: linear-gradient(135deg, #A8E6CF, #56C596); padding: 20px 0;">
    <div class="container is-max-desktop" style="text-align: center;">
      <h2 class="title is-3" style="color: #FFFFFF; margin: 0; font-weight: bold;">Approach Overview</h2>
    </div>
  </div>

<div class="container is-max-desktop" style="margin-top: 20px;">
  <div style="text-align: center;">
    <img src="./static/images/main_fig.png" alt="Approach Overview" 
         style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
  </div>
  <p style="margin-top: 12px; font-size: 0.95em; color: #444; line-height: 1.6; text-align: justify;">
Fig. 1. Overview of the proposed hierarchical framework for language-guided, multimodal, multi-task robotic manipulation.
    The framework operates in three processes:
    (I) Instruction Recognition, where natural language commands are transcribed using a Whisper-based voice-to-text module; 
    (II) High-Level Task Planning, where a VLM first perceives the scene to extract objects and information, and then an LLM, conditioned on the human instruction, the VLM output, and a library of predefined skills, decomposes the command into a sequence of sub-tasks;
    (III) Low-Level Action Generation and Sub-task Switching, where an ACT-based multimodal language-vision policy executes each sub-task by leveraging textual input, visual observations, and robot joint states, while a Task Completion Checker detects termination and triggers the transition to the next sub-task.  
    The right shows two examples of decomposed human instructions and their sub-task executions.
  </p>
</div>



<div class="container is-max-desktop" style="margin-top: 20px;">
  <div style="text-align: center;">
    <img src="./static/images/high_plan.jpg" alt="Two-stage high-level task planner" 
         style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
  </div>
  <p style="margin-top: 12px; font-size: 0.95em; color: #444; line-height: 1.6; text-align: justify;">
    Fig. 2. Two-stage high-level planner: the VLM parses the scene, and the LLM uses it with the human instruction and sub-task library to generate sub-tasks paln list with a brief reply.
  </p>
</div>

<div class="container is-max-desktop" style="margin-top: 20px;">
  <div style="text-align: center;">
    <img src="./static/images/llms_act.jpg" alt="llmsact" 
         style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
  </div>
  <p style="margin-top: 12px; font-size: 0.95em; color: #444; line-height: 1.6; text-align: justify;">
Fig. 3. Architecture of the Low-Level Action Planner. For a sub-task (e.g., “open the balance”), we adopt an asymmetric encoder: the language input is encoded by a frozen SigLIP2 augmented with trainable DoRA adapters, while multi-view images at time t are processed by a fine-tuned ResNet. The resulting features are fused via (i) a temperature-scaled spatial attention for visual enhancement and (ii) a bidirectional cross-attention for language-vision integration. These fused features, together with the robot joint state, are passed to an ACT-based policy, whose output is normalized and augmented with two additional linear heads predicting the next k timesteps of joint actions and their corresponding validity masks. A Task-Completion Checker monitors the masks and terminates the current sub-task if more than n consecutive invalid actions are detected, switching to the next sub-task.
  </p>
</div>


</section>




<hr/>
<script type="text/javascript" src="./static/slick/slick.min.js"></script>
</body>
</html>
